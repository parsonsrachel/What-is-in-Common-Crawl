# What-is-in-Common-Crawl
Machine learning project to investigate what types of data exist in Common Crawl

# September 12
Figured out next steps and what to complete for next class. Started looking at the requirements for the project proposal. Assigned tasks to complete before next class. Add to proposal document with thoughts from literature review. Try querying common crawl to figure out how to use it.

# September 14
Tried pulling data from Common Crawl, but were not yet successful. Added to the literature review. Allocated proposal paper sections with the intention to be done with a first draft of the proposal by next class. 

# September 19
Started the proposal presentation and assigned slides. Still need to edit the paper and fix formating. Still have not figured out how to query Common Crawl.

# September 28
Presented proposal. Notes we got back
- create a filtering method based on the types of data we see
- currated datasets already exist so filtering method that filters out all bad data and not just specific types would be good
- try running some of the filtering methods we looked at to see what is left over from filtering and what was filtered out that maybe shouldn't have been
- won't really have to worry a lot about large data storage because a few thousand is not too large

# Connecting to AWS
```
ssh -i "rachelkey.pem" ec2-user@ec2-3-15-179-239.us-east-2.compute.amazonaws.com
```
